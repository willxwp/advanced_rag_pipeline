{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2ece71",
   "metadata": {},
   "source": [
    "This project aims to build an advanced RAG pipeline that includes quality control mechanisms through relevance checking, which helps mitigate hallucination and improve answer quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e734a",
   "metadata": {},
   "source": [
    "**Indexing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600c7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers import EnsembleRetriever, BM25Retriever\n",
    "from langchain_core.vectorstores.base import VectorStoreRetriever\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3372b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF file\n",
    "loader = PyPDFLoader(\"google-2023-environmental-report.pdf\", mode=\"single\")\n",
    "documents: List[Document] = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede22811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split PDF into 102 chunks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a embedding object\n",
    "load_dotenv()\n",
    "embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Split the documents into chunks\n",
    "chunker = SemanticChunker(embeddings=embedding_function)\n",
    "chunks: List[Document] = chunker.split_documents(documents)\n",
    "print(f\"Split PDF into {len(chunks)} chunks.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34155006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Chroma vector store\n",
    "chroma_client = chromadb.Client()\n",
    "collection_name = \"google_environmental_report\"\n",
    "\n",
    "chroma_vector_store: Chroma = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=\"chroma_db\",\n",
    "    collection_name=collection_name,\n",
    "    client=chroma_client,\n",
    ")\n",
    "\n",
    "# Create a BM25 retriever\n",
    "sparse_retriever: BM25Retriever = BM25Retriever.from_documents(\n",
    "    documents=chunks,\n",
    "    k=5\n",
    ")\n",
    "# Create a Chroma retriever\n",
    "dense_retriever: VectorStoreRetriever = chroma_vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "# Create an ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[dense_retriever, sparse_retriever],\n",
    "    weights=[0.5, 0.5],\n",
    "    c=0, # c=0 means no re-ranking\n",
    "    k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94682af2",
   "metadata": {},
   "source": [
    "**Retrieval and Generation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42914566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.outputs import Generation\n",
    "import json\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e11323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded prompt from hub\n",
      ": input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'jclemens24', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '1a1f3ccb9a5a92363310e3b130843dfb2540239366ebe712ddd94982acc06734'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Load a prompt from the hub\n",
    "# This prompt is used to generate a question-answering prompt for the RAG model\n",
    "prompt = hub.pull(\"jclemens24/rag-prompt\")\n",
    "print(f\"Loaded prompt from hub\\n: {prompt}\")\n",
    "\n",
    "# Create a prompt template for the relevance check\n",
    "relevance_prompt_template: PromptTemplate = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Given the following question and retrieved context, determine if the context is relevant to the question.\n",
    "    Provide a score from 1 to 5, where 1 is not at all relevant and 5 is highly relevant.\n",
    "    Return ONLY the numeric score, without any additional text or explanation.\n",
    "\n",
    "    Question: {question}\n",
    "    Retrieved Context: {retrieved_context}\n",
    "\n",
    "    Relevance Score:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d29b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model that represents the structure of the JSON output:\n",
    "class FinalOutputModel(BaseModel):\n",
    "    relevance_score: float = Field(description=\"The relevance score of the retrieved context to the question\")\n",
    "    answer: str = Field(description=\"The final answer to the question\")\n",
    "\n",
    "\n",
    "# Create an instance of PydanticOutputParser using the FinalOutputModel\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=FinalOutputModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5efa0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs: list[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Format the documents for the input to the context variable.\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "\n",
    "# Define a function to extract the relevance score\n",
    "def extract_relevance_score(llm_output: str) -> float:\n",
    "    \"\"\"\n",
    "    Extract the relevance score from the LLM output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        score = float(llm_output.strip())\n",
    "        return score\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "    \n",
    "\n",
    "# Define a function to get json output using Pydantic parser\n",
    "def format_json_output(x: dict) -> FinalOutputModel | None:\n",
    "    json_output: dict = {\n",
    "        \"relevance_score\": extract_relevance_score(x['relevance_score']),\n",
    "        \"answer\": x['answer'],\n",
    "    }\n",
    "    # uses json.dumps to convert the json_output dict to a JSON string and \n",
    "    # creates a Generation object with the JSON string as its text. \n",
    "    # Finally, uses parse_result() to parse the Generation object and returns the custom pydantic object.\n",
    "    return pydantic_parser.parse_result([Generation(text=json.dumps(json_output))])\n",
    "\n",
    "    \n",
    "# Define a function to get conditional answer with relevance check\n",
    "def conditional_answer(x: dict) -> str | FinalOutputModel | None:\n",
    "    \"\"\"\n",
    "    Given a dictionary with the keys 'question', 'retrieved_context', and 'relevance_score',\n",
    "    return the answer if the relevance score is above a certain threshold.\n",
    "    \"\"\"\n",
    "    relevance_score: float = extract_relevance_score(x['relevance_score'])\n",
    "    if relevance_score < 4:\n",
    "        return \"I don't know due to no relevant content found.\"\n",
    "    else:\n",
    "        return format_json_output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c38681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM and output parser\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "str_output_parser = StrOutputParser()\n",
    "\n",
    "# Build the RAG chain that includes relevance check and answer generation\n",
    "rag_chain = (\n",
    "    RunnableParallel({\"context\": ensemble_retriever, \"question\": RunnablePassthrough()})\n",
    "    | RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"]))) # Update context to a string\n",
    "    | RunnableParallel(\n",
    "        {\n",
    "            \"relevance_score\": (  # Relevance check\n",
    "                RunnablePassthrough()\n",
    "                | (\n",
    "                    lambda x: relevance_prompt_template.format(\n",
    "                        question=x[\"question\"], retrieved_context=x[\"context\"]\n",
    "                    )\n",
    "                )\n",
    "                | llm\n",
    "                | str_output_parser\n",
    "            ),\n",
    "            \"answer\": (  # Answer generation\n",
    "                RunnablePassthrough()\n",
    "                | prompt\n",
    "                | llm\n",
    "                | str_output_parser\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    | RunnablePassthrough().assign(answer=conditional_answer) # Update answer to the custom pydantic object\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad235163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: What are Google's environmental initiatives?\n",
      "\n",
      "Relevance Score: 5.0\n",
      "Final Answer:\n",
      "Google's environmental initiatives include a comprehensive approach to sustainability that focuses on three key pillars: empowering individuals to take action, collaborating with partners and customers, and operating the business sustainably. Some specific initiatives and actions include:\n",
      "\n",
      "1. **Employee Engagement**: Google promotes sustainability within its culture by providing employees with opportunities to engage in environmental issues, participate in sustainability courses, and work on projects like Project Sunroof.\n",
      "\n",
      "2. **Supplier Engagement**: Google works to build a low-carbon, circular supply chain by helping suppliers improve their environmental performance and ensuring compliance with environmental standards through audits and corrective action plans.\n",
      "\n",
      "3. **Public Policy and Advocacy**: Google supports strong public policies that enhance global climate action, aligning with the Paris Agreement and advocating for emissions reduction targets.\n",
      "\n",
      "4. **Sustainability Features in Products**: Google has introduced features like eco-friendly routing in Google Maps, energy efficiency features in Google Nest, and carbon emissions information in Google Flights, aiming to help users make more sustainable choices.\n",
      "\n",
      "5. **Environmental Insights Explorer**: This tool provides cities with data to measure emissions and develop strategies for climate action, making actionable climate data available to over 40,000 cities worldwide.\n",
      "\n",
      "6. **Investment in Renewable Energy**: Google has signed numerous renewable energy agreements, aiming to run on 24/7 carbon-free energy in all its operations.\n",
      "\n",
      "7. **AI and Technology for Sustainability**: Google leverages AI to address environmental challenges, such as predicting floods and wildfires, optimizing traffic to reduce emissions, and enhancing urban planning with tools like cool roofs.\n",
      "\n",
      "8. **Community Engagement**: Google engages with individuals and communities through campaigns and educational resources, such as the Non-Fungible Planet campaign and providing information on recycling and sustainable practices.\n",
      "\n",
      "Overall, Google's initiatives reflect a commitment to reducing its environmental impact while empowering others to do the same through technology and collaboration.\n",
      "\n",
      "Final JSON Output:\n",
      "{'relevance_score': '5', 'answer': FinalOutputModel(relevance_score=5.0, answer=\"Google's environmental initiatives include a comprehensive approach to sustainability that focuses on three key pillars: empowering individuals to take action, collaborating with partners and customers, and operating the business sustainably. Some specific initiatives and actions include:\\n\\n1. **Employee Engagement**: Google promotes sustainability within its culture by providing employees with opportunities to engage in environmental issues, participate in sustainability courses, and work on projects like Project Sunroof.\\n\\n2. **Supplier Engagement**: Google works to build a low-carbon, circular supply chain by helping suppliers improve their environmental performance and ensuring compliance with environmental standards through audits and corrective action plans.\\n\\n3. **Public Policy and Advocacy**: Google supports strong public policies that enhance global climate action, aligning with the Paris Agreement and advocating for emissions reduction targets.\\n\\n4. **Sustainability Features in Products**: Google has introduced features like eco-friendly routing in Google Maps, energy efficiency features in Google Nest, and carbon emissions information in Google Flights, aiming to help users make more sustainable choices.\\n\\n5. **Environmental Insights Explorer**: This tool provides cities with data to measure emissions and develop strategies for climate action, making actionable climate data available to over 40,000 cities worldwide.\\n\\n6. **Investment in Renewable Energy**: Google has signed numerous renewable energy agreements, aiming to run on 24/7 carbon-free energy in all its operations.\\n\\n7. **AI and Technology for Sustainability**: Google leverages AI to address environmental challenges, such as predicting floods and wildfires, optimizing traffic to reduce emissions, and enhancing urban planning with tools like cool roofs.\\n\\n8. **Community Engagement**: Google engages with individuals and communities through campaigns and educational resources, such as the Non-Fungible Planet campaign and providing information on recycling and sustainable practices.\\n\\nOverall, Google's initiatives reflect a commitment to reducing its environmental impact while empowering others to do the same through technology and collaboration.\")}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Invoke the RAG chain with a user query\n",
    "user_query = \"What are Google's environmental initiatives?\"\n",
    "result: dict[str, Any] = rag_chain.invoke(user_query)\n",
    "\n",
    "print(f\"Original Question: {user_query}\\n\")\n",
    "print(f\"Relevance Score: {result['answer'].relevance_score}\") \n",
    "print(f\"Final Answer:\\n{result['answer'].answer}\\n\")\n",
    "print(f\"Final JSON Output:\\n{result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a4dc9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: How is Google developing its LLM?\n",
      "\n",
      "Final JSON Output:\n",
      "{'relevance_score': '1', 'answer': \"I don't know due to no relevant content found.\"}\n"
     ]
    }
   ],
   "source": [
    "user_query = \"How is Google developing its LLM?\"\n",
    "result: dict[str, Any] = rag_chain.invoke(user_query)\n",
    "\n",
    "print(f\"Original Question: {user_query}\\n\")\n",
    "print(f\"Final JSON Output:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f2ee0",
   "metadata": {},
   "source": [
    "**UI for Testing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ddd90e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio    \n",
    "import nest_asyncio  \n",
    "import gradio as gr\n",
    " \n",
    "asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())  # set the event loop policy to the default policy\n",
    "nest_asyncio.apply()  # apply the necessary patches to enable nested event loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88cd85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to handle the user input and generate the response.\n",
    "def process_user_input(question: str) -> tuple:\n",
    "    \"\"\"Process the user input and generate the response using the RAG chain.\"\"\"\n",
    "\n",
    "    # Run the RAG chain with the user question and retrieve the answer and sources.\n",
    "    result: dict[str, Any] = rag_chain.invoke(question)\n",
    "    \n",
    "    # Extract the answer and sources from the result.\n",
    "    if isinstance(result[\"answer\"], FinalOutputModel):\n",
    "        final_answer: str = result[\"answer\"].answer\n",
    "        relevance_score: float = result[\"answer\"].relevance_score\n",
    "    else:\n",
    "        final_answer: str = result[\"answer\"]\n",
    "        relevance_score: float = float(result[\"relevance_score\"])\n",
    "\n",
    "    return relevance_score, final_answer\n",
    "\n",
    "\n",
    "#  Next set up an instance of the Gradio interface:\n",
    "demo = gr.Interface(\n",
    "    fn=process_user_input,  # The function to be called when the user submits input.\n",
    "    inputs=gr.Textbox(\n",
    "        label=\"Enter your question\",  \n",
    "        value=\"What are Google's environmental initiatives?\" # The default value for the input component.\n",
    "    ),\n",
    "\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Relevance Score\"),  \n",
    "        gr.Textbox(label=\"Final Answer\"),  \n",
    "    ],\n",
    "\n",
    "    title=\"RAG Question Answering\",  # The title of the Gradio interface.\n",
    "    description=\"Enter a question about Google's 2023 environmental report and get an answer and associated relevance score.\",  \n",
    "    theme=\"default\",  # The theme for the Gradio interface.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb4c4219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://4bf96ef078fc55d27f.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4bf96ef078fc55d27f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://4bf96ef078fc55d27f.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
